{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Scalable ML 01\n",
    "To visit the original guide go to [link](https://tomaugspurger.github.io/scalable-ml-01).\n",
    "\n",
    "The goal here is to fight the memory and time constraints when datasets grow bigger than RAM and the data starts to be a problem.\n",
    "\n",
    "**Citing the author:**\n",
    "1. **I'm constrained by size:** My training dataset fits in RAM, but I have to predict for a much larger dataset. Or, my training dataset doesn't even fit in RAM. I'd like to scale out by adopting algorithms that work in batches locally, or on a distributed cluster.\n",
    "2. **I'm constrained by time:** I'd like to fit more models (think hyper-parameter optimization or ensemble learning) on my dataset in a given amount of time. I'd like to scale out by fitting more models in parallel, either on my laptop by using more cores, or on a cluster.\n",
    "\n",
    "These aren't mutually exclusive or exhaustive, but they should serve as a nice framework for our discussion. I'll be showing where the usual pandas + scikit-learn for in-memory analytics workflow breaks down, and offer some solutions for scaling out to larger problems.\n",
    "\n",
    "This post will focus on cases where your training dataset fits in memory, but you must predict on a dataset that's larger than memory. Later posts will explore into parallel, out-of-core, and distributed training of machine learning models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data in-memory Storage Interface\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "Sklearn\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "# We'll use FunctionTransformer for simple transforms\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "# TransformerMixin gives us fit_transform for free\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\"\"\"\n",
    "Jupyter Notebook\n",
    "\"\"\"\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\"\"\"\n",
    "Jupyter Notebook\n",
    "\"\"\"\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\"\"\"\n",
    "Pandas\n",
    "\"\"\"\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "First we need to download the dataset for the test we are performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load download.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data file structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 solid  staff   1.6G Dec  6 19:45 data/yellow_tripdata_2009-01.csv\r\n",
      "-rw-r--r--  1 solid  staff   1.5G Dec  6 19:45 data/yellow_tripdata_2009-02.csv\r\n",
      "-rw-r--r--  1 solid  staff   1.5G Dec  6 19:45 data/yellow_tripdata_2009-03.csv\r\n",
      "-rw-r--r--  1 solid  staff   1.3G Dec  6 19:45 data/yellow_tripdata_2009-04.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh data/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load the first DataFrame into Memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor_name</th>\n",
       "      <th>Trip_Pickup_DateTime</th>\n",
       "      <th>Trip_Dropoff_DateTime</th>\n",
       "      <th>Passenger_Count</th>\n",
       "      <th>Trip_Distance</th>\n",
       "      <th>Start_Lon</th>\n",
       "      <th>Start_Lat</th>\n",
       "      <th>Rate_Code</th>\n",
       "      <th>store_and_forward</th>\n",
       "      <th>End_Lon</th>\n",
       "      <th>End_Lat</th>\n",
       "      <th>Payment_Type</th>\n",
       "      <th>Fare_Amt</th>\n",
       "      <th>surcharge</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>Tip_Amt</th>\n",
       "      <th>Tolls_Amt</th>\n",
       "      <th>Total_Amt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VTS</td>\n",
       "      <td>2009-01-04 02:52:00</td>\n",
       "      <td>2009-01-04 03:02:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2.63</td>\n",
       "      <td>-73.991957</td>\n",
       "      <td>40.721567</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-73.993803</td>\n",
       "      <td>40.695922</td>\n",
       "      <td>CASH</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VTS</td>\n",
       "      <td>2009-01-04 03:31:00</td>\n",
       "      <td>2009-01-04 03:38:00</td>\n",
       "      <td>3</td>\n",
       "      <td>4.55</td>\n",
       "      <td>-73.982102</td>\n",
       "      <td>40.736290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-73.955850</td>\n",
       "      <td>40.768030</td>\n",
       "      <td>Credit</td>\n",
       "      <td>12.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VTS</td>\n",
       "      <td>2009-01-03 15:43:00</td>\n",
       "      <td>2009-01-03 15:57:00</td>\n",
       "      <td>5</td>\n",
       "      <td>10.35</td>\n",
       "      <td>-74.002587</td>\n",
       "      <td>40.739748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-73.869983</td>\n",
       "      <td>40.770225</td>\n",
       "      <td>Credit</td>\n",
       "      <td>23.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DDS</td>\n",
       "      <td>2009-01-01 20:52:58</td>\n",
       "      <td>2009-01-01 21:14:00</td>\n",
       "      <td>1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-73.974267</td>\n",
       "      <td>40.790955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-73.996558</td>\n",
       "      <td>40.731849</td>\n",
       "      <td>CREDIT</td>\n",
       "      <td>14.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DDS</td>\n",
       "      <td>2009-01-24 16:18:23</td>\n",
       "      <td>2009-01-24 16:24:56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-74.001580</td>\n",
       "      <td>40.719382</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-74.008378</td>\n",
       "      <td>40.720350</td>\n",
       "      <td>CASH</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vendor_name Trip_Pickup_DateTime Trip_Dropoff_DateTime  Passenger_Count  \\\n",
       "0         VTS  2009-01-04 02:52:00   2009-01-04 03:02:00                1   \n",
       "1         VTS  2009-01-04 03:31:00   2009-01-04 03:38:00                3   \n",
       "2         VTS  2009-01-03 15:43:00   2009-01-03 15:57:00                5   \n",
       "3         DDS  2009-01-01 20:52:58   2009-01-01 21:14:00                1   \n",
       "4         DDS  2009-01-24 16:18:23   2009-01-24 16:24:56                1   \n",
       "\n",
       "   Trip_Distance  Start_Lon  Start_Lat  Rate_Code  store_and_forward  \\\n",
       "0           2.63 -73.991957  40.721567        NaN                NaN   \n",
       "1           4.55 -73.982102  40.736290        NaN                NaN   \n",
       "2          10.35 -74.002587  40.739748        NaN                NaN   \n",
       "3           5.00 -73.974267  40.790955        NaN                NaN   \n",
       "4           0.40 -74.001580  40.719382        NaN                NaN   \n",
       "\n",
       "     End_Lon    End_Lat Payment_Type  Fare_Amt  surcharge  mta_tax  Tip_Amt  \\\n",
       "0 -73.993803  40.695922         CASH       8.9        0.5      NaN     0.00   \n",
       "1 -73.955850  40.768030       Credit      12.1        0.5      NaN     2.00   \n",
       "2 -73.869983  40.770225       Credit      23.7        0.0      NaN     4.74   \n",
       "3 -73.996558  40.731849       CREDIT      14.9        0.5      NaN     3.05   \n",
       "4 -74.008378  40.720350         CASH       3.7        0.0      NaN     0.00   \n",
       "\n",
       "   Tolls_Amt  Total_Amt  \n",
       "0        0.0       9.40  \n",
       "1        0.0      14.60  \n",
       "2        0.0      28.44  \n",
       "3        0.0      18.45  \n",
       "4        0.0       3.70  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.3 s, sys: 5.2 s, total: 58.5 s\n",
      "Wall time: 59.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dtype = {\n",
    "    'vendor_name': 'category',\n",
    "    'Payment_Type': 'category',\n",
    "}\n",
    "\n",
    "df = pd.read_csv(\"data/yellow_tripdata_2009-01.csv\", dtype=dtype,\n",
    "                 parse_dates=['Trip_Pickup_DateTime', 'Trip_Dropoff_DateTime'],)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our X and y to the task at hand. Taking into account that we want to predict the Amount of Tip a client is going to pay, we create the following Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Tip_Amt\", axis=1)\n",
    "y = df['Tip_Amt'] > 0\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the size of the obtained folds, per se, train and test size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sample size: 7073603\n",
      "test sample size: 2357868\n"
     ]
    }
   ],
   "source": [
    "print('train sample size: {}\\ntest sample size: {}'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix the `Payment_Type`, since there are some incosistensies in this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            CASH\n",
       "1          Credit\n",
       "3          CREDIT\n",
       "9            Cash\n",
       "96      No Charge\n",
       "1167      Dispute\n",
       "Name: Payment_Type, dtype: category\n",
       "Categories (6, object): [CASH, CREDIT, Cash, Credit, Dispute, No Charge]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Payment_Type'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            cash\n",
       "1          credit\n",
       "2          credit\n",
       "3          credit\n",
       "4            cash\n",
       "            ...  \n",
       "9431466      cash\n",
       "9431467    credit\n",
       "9431468    credit\n",
       "9431469      cash\n",
       "9431470      cash\n",
       "Name: Payment_Type, Length: 9431471, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Payment_Type'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Pipeline\n",
    "**Citing the author in this one:**\n",
    "* The downside of this approach is that we now have to remember which pre-processing steps we did, and in what order. The pipeline from raw data to fit model is spread across multiple python objects. **A better approach is to use scikit-learn's Pipeline object.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = [\n",
    "    Imputer(strategy='median'), \n",
    "    StandardScaler(), \n",
    "    LinearRegression()\n",
    "]\n",
    "pipeline = make_pipeline(*sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit our data into the pipeline and check if it works fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But remember, we have one problem only. How do we do the lowercase() transformation in new data? Do not worry, we can add it to the pipeline. Check the code bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def payment_lowerer(X):\n",
    "    return X.assign(Payment_Type=X.Payment_Type.str.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not need to use all the classes in this dataset, so a column selector would be awesome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelector(TransformerMixin):\n",
    "    \"Select `columns` from `X`\"\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Citing the author:**\n",
    "* Internally, pandas stores datetimes like Trip_Pickup_DateTime as a 64-bit integer representing the nanoseconds since some time in the 1600s. If we left this untransformed, scikit-learn would happily transform that column to its integer representation, which may not be the most meaningful item to stick in a linear model for predicting tips. A better feature might the hour of the day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HourExtractor(TransformerMixin):\n",
    "    \"Transform each datetime in `columns` to integer hour of the day\"\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.assign(**{col: lambda x: x[col].dt.hour\n",
    "                           for col in self.columns})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Citing the author:**\n",
    "* Likewise, we'll need to ensure the categorical variables (in a statistical sense) are categorical dtype (in a pandas sense). We want categoricals so that we can call get_dummies later on without worrying about missing or extra categories in a subset of the data throwing off our linear algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalEncoder(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Convert to Categorical with specific `categories`.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> CategoricalEncoder({\"A\": ['a', 'b', 'c']}).fit_transform(\n",
    "    ...     pd.DataFrame({\"A\": ['a', 'b', 'a', 'a']})\n",
    "    ... )['A']\n",
    "    0    a\n",
    "    1    b\n",
    "    2    a\n",
    "    3    a\n",
    "    Name: A, dtype: category\n",
    "    Categories (2, object): [a, b, c]\n",
    "    \"\"\"\n",
    "    def __init__(self, categories):\n",
    "        self.categories = categories\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        for col, categories in self.categories.items():\n",
    "            X[col] = X[col].astype('category').cat.set_categories(categories)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Citing the author:**\n",
    "* Finally, we'd like to normalize the quantitative subset of the data. Scikit-learn has a StandardScaler, which we'll mimic here, to just operate on a subset of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler(TransformerMixin):\n",
    "    \"Scale a subset of the columns in a DataFrame\"\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Yes, non-ASCII symbols can be a valid identfiers in python 3\n",
    "        self.μs = X[self.columns].mean()\n",
    "        self.σs = X[self.columns].std()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X[self.columns] = X[self.columns].sub(self.μs).div(self.σs)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time to build the pipeline**, let's define everything now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns at the start of the pipeline\n",
    "columns = ['vendor_name', 'Trip_Pickup_DateTime',\n",
    "           'Passenger_Count', 'Trip_Distance',\n",
    "           'Payment_Type', 'Fare_Amt', 'surcharge']\n",
    "\n",
    "# The mapping of {column: set of categories}\n",
    "categories = {\n",
    "    'vendor_name': ['CMT', 'DDS', 'VTS'],\n",
    "    'Payment_Type': ['cash', 'credit', 'dispute', 'no charge'],\n",
    "}\n",
    "\n",
    "scale = ['Trip_Distance', 'Fare_Amt', 'surcharge']\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    ColumnSelector(columns),\n",
    "    HourExtractor(['Trip_Pickup_DateTime']),\n",
    "    FunctionTransformer(payment_lowerer, validate=False),\n",
    "    CategoricalEncoder(categories),\n",
    "    FunctionTransformer(pd.get_dummies, validate=False),\n",
    "    StandardScaler(scale),\n",
    "    Imputer(strategy='median'),\n",
    "    LogisticRegression(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('columnselector', <__main__.ColumnSelector at 0x17c33c8d0>),\n",
       " ('hourextractor', <__main__.HourExtractor at 0x17c33cf98>),\n",
       " ('functiontransformer-1', FunctionTransformer(accept_sparse=False,\n",
       "            func=<function payment_lowerer at 0x18cd47488>, inv_kw_args=None,\n",
       "            inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "            validate=False)),\n",
       " ('categoricalencoder', <__main__.CategoricalEncoder at 0x17c33c630>),\n",
       " ('functiontransformer-2', FunctionTransformer(accept_sparse=False,\n",
       "            func=<function get_dummies at 0x105f28378>, inv_kw_args=None,\n",
       "            inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "            validate=False)),\n",
       " ('standardscaler', <__main__.StandardScaler at 0x17c33cdd8>),\n",
       " ('imputer',\n",
       "  Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)),\n",
       " ('logisticregression',\n",
       "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.7 s, sys: 5.91 s, total: 1min 3s\n",
      "Wall time: 1min 7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('columnselector', <__main__.ColumnSelector object at 0x17c33c8d0>), ('hourextractor', <__main__.HourExtractor object at 0x17c33cf98>), ('functiontransformer-1', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function payment_lowerer at 0x18cd47488>, inv_kw_args=None,\n",
       "          inve...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.97 s, sys: 1.72 s, total: 5.69 s\n",
      "Wall time: 5.66 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.99310506965120882"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.3 s, sys: 488 ms, total: 1.79 s\n",
      "Wall time: 1.78 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.99313320338543121"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out people essentially tip if and only if they're paying with a card, so this isn't a particularly difficult task. Or perhaps more accurately, tips are only recorded when someone pays with a card."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalling Out with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we've fit our model and it's been basically normal. Maybe we've been overly-dogmatic about doing everything in a pipeline, but that's just good model hygiene anyway.\n",
    "\n",
    "Now, to scale out to the rest of the dataset. We'll predict the probability of tipping for every cab ride in the dataset (bearing in mind that the full dataset doesn't fit in my laptop's RAM, so we'll do it out-of-core).\n",
    "\n",
    "To make things a bit easier we'll use dask, though it isn't strictly necessary for this section. It saves us from writing a for loop (big whoop). Later on well see that we can, reuse this code when we go to scale out to a cluster (that part is pretty cool, actually). Dask can scale down to a single laptop, and up to thousands of cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv(\"data/*.csv\", dtype=dtype,\n",
    "                 parse_dates=['Trip_Pickup_DateTime', 'Trip_Dropoff_DateTime'],)\n",
    "\n",
    "X = df.drop(\"Tip_Amt\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = X.map_partitions(lambda x: pd.Series(pipe.predict_proba(x)[:, 1],\n",
    "                                            name='yhat'),\n",
    "                        meta=('yhat', 'f8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have a mighty dataset with 34M points, WTF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 54s, sys: 38.9 s, total: 5min 33s\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%time yhat.to_frame().to_parquet(\"data/predictions.parq\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
